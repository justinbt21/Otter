{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88005fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/justintran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/justintran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/justintran/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/justintran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "/var/folders/6m/5k3bjd9x18s4c586_k1j2jzh0000gn/T/ipykernel_99013/1780935594.py:115: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df2.loc[df2['item_type_new'].str.contains(v, regex=True), k] = 1\n",
      "/var/folders/6m/5k3bjd9x18s4c586_k1j2jzh0000gn/T/ipykernel_99013/1780935594.py:115: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df2.loc[df2['item_type_new'].str.contains(v, regex=True), k] = 1\n",
      "/var/folders/6m/5k3bjd9x18s4c586_k1j2jzh0000gn/T/ipykernel_99013/1780935594.py:115: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df2.loc[df2['item_type_new'].str.contains(v, regex=True), k] = 1\n",
      "/var/folders/6m/5k3bjd9x18s4c586_k1j2jzh0000gn/T/ipykernel_99013/1780935594.py:115: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df2.loc[df2['item_type_new'].str.contains(v, regex=True), k] = 1\n",
      "/var/folders/6m/5k3bjd9x18s4c586_k1j2jzh0000gn/T/ipykernel_99013/1780935594.py:115: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df2.loc[df2['item_type_new'].str.contains(v, regex=True), k] = 1\n",
      "/var/folders/6m/5k3bjd9x18s4c586_k1j2jzh0000gn/T/ipykernel_99013/1780935594.py:115: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df2.loc[df2['item_type_new'].str.contains(v, regex=True), k] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting XGB Model to data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justintran/opt/anaconda3/envs/otter_takehom/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/justintran/opt/anaconda3/envs/otter_takehom/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB F1 for train for is                precision    recall  f1-score   support\n",
      "\n",
      "      italian       1.00      0.93      0.96      5445\n",
      "   vietnamese       1.00      0.96      0.98       663\n",
      "       korean       1.00      0.10      0.18       139\n",
      "       indian       0.99      0.86      0.92      2106\n",
      "     southern       1.00      0.95      0.97      2684\n",
      "mediterranean       1.00      0.93      0.96      2400\n",
      "    breakfast       1.00      0.96      0.98     11412\n",
      "     american       1.00      0.96      0.98     15640\n",
      "      chinese       1.00      0.86      0.92      1922\n",
      "     japanese       0.99      0.89      0.94      3111\n",
      "        latin       1.00      0.90      0.95      7049\n",
      "         thai       0.97      0.96      0.96      2014\n",
      "   sandwiches       1.00      1.00      1.00      2618\n",
      "         soup       1.00      1.00      1.00       418\n",
      "       coffee       1.00      0.93      0.96       656\n",
      "       drinks       1.00      1.00      1.00      3410\n",
      "     hawaiian       1.00      0.56      0.71       333\n",
      "      healthy       1.00      0.97      0.98      4156\n",
      "       sweets       1.00      0.77      0.87      1211\n",
      "      seafood       0.98      0.89      0.93       836\n",
      "         rice       1.00      1.00      1.00       626\n",
      "\n",
      "    micro avg       1.00      0.94      0.97     68849\n",
      "    macro avg       1.00      0.87      0.91     68849\n",
      " weighted avg       1.00      0.94      0.96     68849\n",
      "  samples avg       0.94      0.93      0.93     68849\n",
      "\n",
      "XGB F1 for test for is                precision    recall  f1-score   support\n",
      "\n",
      "      italian       1.00      0.93      0.96      3650\n",
      "   vietnamese       1.00      0.97      0.98       490\n",
      "       korean       1.00      0.07      0.12        90\n",
      "       indian       1.00      0.87      0.93      1451\n",
      "     southern       1.00      0.95      0.97      1732\n",
      "mediterranean       1.00      0.91      0.95      1581\n",
      "    breakfast       1.00      0.96      0.98      7554\n",
      "     american       1.00      0.96      0.98     10191\n",
      "      chinese       1.00      0.86      0.92      1176\n",
      "     japanese       0.98      0.89      0.94      2106\n",
      "        latin       1.00      0.90      0.95      4785\n",
      "         thai       0.97      0.94      0.96      1371\n",
      "   sandwiches       1.00      1.00      1.00      1633\n",
      "         soup       1.00      1.00      1.00       291\n",
      "       coffee       1.00      0.95      0.97       477\n",
      "       drinks       1.00      1.00      1.00      2238\n",
      "     hawaiian       1.00      0.53      0.69       244\n",
      "      healthy       1.00      0.95      0.97      2717\n",
      "       sweets       1.00      0.78      0.88       781\n",
      "      seafood       0.99      0.88      0.93       629\n",
      "         rice       1.00      1.00      1.00       417\n",
      "\n",
      "    micro avg       1.00      0.94      0.97     45604\n",
      "    macro avg       1.00      0.87      0.91     45604\n",
      " weighted avg       1.00      0.94      0.96     45604\n",
      "  samples avg       0.94      0.93      0.93     45604\n",
      "\n",
      "XGB Hamming Loss is 0.0041242066093159025\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ux' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m x \u001b[38;5;241m=\u001b[39m unlabel_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_type_new\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    155\u001b[0m uy \u001b[38;5;241m=\u001b[39m mlb\u001b[38;5;241m.\u001b[39mtransform(unlabel_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m xgb_pipe\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mux\u001b[49m)\n\u001b[1;32m    158\u001b[0m y_pred_tags \u001b[38;5;241m=\u001b[39m mlb\u001b[38;5;241m.\u001b[39minverse_transform(y_pred)\n\u001b[1;32m    160\u001b[0m y_tags \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, y_pred_tags), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_tags\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ux' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import LdaModel, Word2Vec, Doc2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict  # For word frequency\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import warnings\n",
    "from sklearn.metrics import hamming_loss\n",
    "from pandarallel import pandarallel\n",
    "from thefuzz import process, fuzz\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "pandarallel.initialize()\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def find_root_item_type(word, word_list, limit, min_score=90):\n",
    "    _words = process.extract(word, word_list, scorer=fuzz.token_sort_ratio, limit=limit)\n",
    "    score_words = list(filter(lambda x: x[1] >= min_score, _words))\n",
    "    score_words.sort(key=lambda x: len(x[0]) / (x[1]))\n",
    "    filtered_words = [x[0] for x in score_words]\n",
    "    if len(filtered_words) > 1:\n",
    "        filtered_word = filtered_words[0]\n",
    "    else:\n",
    "        filtered_word = word\n",
    "\n",
    "    return filtered_word\n",
    "\n",
    "\n",
    "def clean_text(line):\n",
    "    clean1 = re.sub(r\"\\b(oz|ml|(\\d\\w)|on|pc|combo|liter)\\b|(\\(.+\\))|[^A-Za-z\\s]\", \"\", line)\n",
    "    #    clean2=re.sub(r\"\", \"\", clean1)\n",
    "    clean_stop = \" \".join([word for word in clean1.split() if word not in stop_words])\n",
    "\n",
    "    return clean_stop\n",
    "\n",
    "raw_df = pd.read_csv('./raw.csv')\n",
    "\n",
    "df = raw_df.copy()\n",
    "df['total_eater_revenue'] = df[['total_eater_spend','total_eater_discount']].sum(axis=1)\n",
    "df['total_orders_promo'] = df['first_time_orders_promo'] + df['returning_orders_promo']\n",
    "df['completion_rate'] = df['completed_orders'] / df['accepted_orders']\n",
    "df['acceptance_rate'] = df['accepted_orders'] / df['requested_orders']\n",
    "df['avg_prep_time_min'] = df['avg_prep_time'] / 60.0\n",
    "df['spend_per_prep_min'] = df['total_eater_spend'] / (df['avg_prep_time']*df['accepted_orders']*1.0)\n",
    "df['total_eater_revenue'] = df[['total_eater_spend','total_eater_discount']].sum(axis=1)\n",
    "# preprocess text by removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['name'] = df['name'].str.lower()\n",
    "df['clean_name'] = df['name'].apply(clean_text)\n",
    "\n",
    "desc_regex=r\"(\\b(spicy|^classic|ultimate|signature)\\b)|(french|fresh cut|home)\\s(?<!fries)\"\n",
    "df['item_type'] = df['clean_name'].apply(lambda x: \" \".join(w for w in re.sub(desc_regex, \"\", x).split())).tolist()\n",
    "df.loc[df.item_type == '', 'item_type'] = df['clean_name']\n",
    "all_items = set(df['item_type'].tolist())\n",
    "\n",
    "# create food item dictionary\n",
    "item_df = df[['item_type']].drop_duplicates()\n",
    "# Fuzzy match dictionary on self to consolidate item types\n",
    "item_df['item_type_new'] = item_df.parallel_apply(lambda x: find_root_item_type(x['item_type'], all_items, min_score=90, limit=6), axis=1)\n",
    "\n",
    "df2 = df.merge(item_df, on='item_type')\n",
    "\n",
    "cuisine_t={\n",
    "    'italian':r'fettucini|rigatoni|lasagna|spaghetti|penne|gnocchi|tortellini|pasta|carbonara|pizza|calzone|garlic bread|alfredo|mozzarella|caesar|cacio|fe[t]{1,2}u[c]{1,2}ine|ravioli|burrata|proscuito|chicken parm|alfredo',\n",
    "    'vietnamese': r'\\b(pho)\\b|spring roll|vietnamese|\\b(ba[nh]{2})\\b mi|thit nuong|cha gio|\\bcuon\\b|summer roll',\n",
    "    'korean': r'bibimbap|korean|kimchi',\n",
    "    'indian': r'paneer|tikka|masala|indian|pakora|gobhi|samosa|naan|basmati|lassi|saag|biryani|makhni|vindaloo|tandoori|korma|butter chicken|dolma',\n",
    "    'southern': r'fried chicken|gumbo|brisket|smoke|bbq|fried zucchini|bbq|coleslaw',\n",
    "    'mediterranean': r'pita|tabouleh|fattoush|gyro|kebab|kabob|skewer|falafel|greek|kofta|shawarma|hummus|tzatziki',\n",
    "    'breakfast': r'orange juice|egg|breakfast|bagel|toast|bacon|omelette|croissant|hash brown|lox|waffle|pancake|sausage',\n",
    "    'american': r'mac.*cheese|burger|\\bwing[s]?\\b|bacon|reuben|cheesesteak|tater tots|fries|buffalo|ranch|onion rings|grilled cheese|melt|nashville|slider|chili cheese|garlic knots|tender|nuggets|dog',\n",
    "    'chinese': r'orange chicken|tofu|chinese|mein|dumplings|mongolian|potsticker|fried rice|general tsos|wontons|chow fun|szechuan|beef broccoli|kung pao|\\b(beef broccoli)\\b',\n",
    "    'japanese': r'ramen|sushi|sashimi|nigiri|unagi|katsu|((?<!egg)(?<!spring)(?<!lobster)(?<!lamb)(?<!curry)(?<!cinnamon)\\sroll)|gyoza|tempura|miso|edamame|udon|wasabi|karaage|teriyaki|\\bsoba\\b',\n",
    "    'latin': r'mexican|taco|burrito|guac|chorizo|al pastor|quesadilla|salsa|birria|horchata|carne asada|el verde|refried beans|tostada|nachos|churro|arepa|empanada|tortillas|jerk|caribbean',\n",
    "    'thai': r'panang|pad thai|pad see ew|\\bthai\\b|drunken noodle|((red|yellow|green)\\scurry)|tom kha|massaman|satay',\n",
    "    'sandwiches': r'sandwich|blt|turkey club|roast beef',\n",
    "    'soup': r'(soup)',\n",
    "    'coffee': r'latte|capuccino|coffee|cappucino|cold brew',\n",
    "    'drinks': r'water|coke|sprite|ginger ale|lemonade|pepsi|juice|\\b(tea)\\b|gatorade',\n",
    "    'hawaiian': r'hawaiian|poke|musubi',\n",
    "    'healthy': r'salad|juice|healthy|fruit|acai|berry|vegan|vegetables|veggies|smoothie',\n",
    "    'sweets': r'waffle|ice cream|tiramisu|oreo|cinnamon roll|cheesecake|smoothie|donuts|chocolate|cookie|caramel|pudding',\n",
    "    'seafood': r'fish|lobster|crab|shrimp',\n",
    "    'rice': r'rice bowl|white rice',\n",
    "}\n",
    "\n",
    "cuisine_list = list(cuisine_t.keys())\n",
    "for k,v in cuisine_t.items():\n",
    "    df2.loc[df2['item_type_new'].str.contains(v, regex=True), k] = 1\n",
    "\n",
    "df2[cuisine_list] = df2[cuisine_list].fillna(0)\n",
    "df2['sum'] = df2[cuisine_list].sum(axis=1)\n",
    "df2.loc[df2['sum'] == 0].shape\n",
    "\n",
    "df2['tags'] = df2[cuisine_list].gt(0).apply(lambda x: x.index[x].tolist(), axis=1)\n",
    "df2.loc[df2.item_type_new.str.contains('wing')][['clean_name','item_type','item_type_new','tags']]\n",
    "\n",
    "valid_df = df2.loc[df2['sum'] >= 1]\n",
    "unlabel_df = df2.loc[df2['sum'] == 0].reset_index(drop=True)\n",
    "unlabel_df['tags'] = unlabel_df[list(cuisine_t.keys())].gt(0).apply(lambda x: x.index[x].tolist(), axis=1)\n",
    "\n",
    "xgb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(strip_accents='unicode', analyzer='word', max_features=200)),\n",
    "    ('clf', OneVsRestClassifier(GradientBoostingClassifier()))\n",
    "])\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=cuisine_list)\n",
    "mlb.fit(cuisine_list)\n",
    "\n",
    "X = valid_df['item_type_new']\n",
    "y = mlb.transform(valid_df['tags'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=1)\n",
    "\n",
    "model_list = {'XGB': xgb_pipe}  # ,'Logistic': logi_pipe,'Naive Bayes': nb_pipe}\n",
    "\n",
    "for k, v in model_list.items():\n",
    "    print(f\"Fitting {k} Model to data\")\n",
    "    v.fit(x_train, y_train)\n",
    "\n",
    "    test_predict = v.predict(x_test)\n",
    "    train_predict = v.predict(x_train)\n",
    "    print(\n",
    "        f'{k} F1 for train for is {classification_report(y_train, train_predict, target_names=list(cuisine_t.keys()))}')\n",
    "    print(f'{k} F1 for test for is {classification_report(y_test, test_predict, target_names=list(cuisine_t.keys()))}')\n",
    "    print(f'{k} Hamming Loss is {hamming_loss(y_test, test_predict)}')\n",
    "\n",
    "ux = unlabel_df['item_type_new']\n",
    "uy = mlb.transform(unlabel_df['tags'])\n",
    "\n",
    "y_pred = xgb_pipe.predict(ux)\n",
    "y_pred_tags = mlb.inverse_transform(y_pred)\n",
    "\n",
    "y_tags = pd.Series(map(list, y_pred_tags), name='pred_tags')\n",
    "pred_df = unlabel_df.merge(y_tags, left_index=True, right_index=True)\n",
    "\n",
    "for i in cuisine_list:\n",
    "    pred_df[i] = pred_df.apply(lambda x: 1 if i in x['pred_tags'] else 0, axis = 1)\n",
    "\n",
    "pred_df['sum'] = pred_df[cuisine_list].sum(axis=1)\n",
    "\n",
    "final_df = pd.concat([valid_df, unlabel_df])\n",
    "final_pred_df = pd.concat([valid_df, pred_df])\n",
    "if (final_df.shape[0] == raw_df.shape[0]) & (final_pred_df.shape[0] == raw_df.shape[0]):\n",
    "    final_df.to_csv('data.csv', header=True, index=False)\n",
    "    final_pred_df.to_csv('pred_data.csv', header=True, index=False)\n",
    "else:\n",
    "    raise ValueError('Rows didnt match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b100f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
